@article{Vaswani2017,
   abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
   author = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Łukasz Kaiser and Illia Polosukhin},
   isbn = {1706.03762v7},
   issn = {10495258},
   journal = {Advances in Neural Information Processing Systems},
   month = {6},
   pages = {5999-6009},
   publisher = {Neural information processing systems foundation},
   title = {Attention Is All You Need},
   volume = {2017-December},
   url = {https://arxiv.org/abs/1706.03762v7},
   year = {2017},
}
@inproceedings{Smith2007,
   abstract = {The Tesseract OCR engine, as was the HP Research Prototype in the UNLV Fourth Annual Test of OCR Accuracy[1], is described in a comprehensive overview. Emphasis is placed on aspects that are novel or at least unusual in an OCR engine, including in particular the line finding, features/classification methods, and the adaptive classifier.},
   author = {R. Smith},
   doi = {10.1109/ICDAR.2007.4376991},
   isbn = {0-7695-2822-8},
   issn = {1520-5363},
   booktitle = {Ninth International Conference on Document Analysis and Recognition (ICDAR 2007) Vol 2},
   month = {9},
   pages = {629-633},
   publisher = {IEEE},
   title = {An Overview of the Tesseract OCR Engine},
   url = {http://ieeexplore.ieee.org/document/4376991/},
   year = {2007},
}
@inproceedings{Neudecker2021,
   abstract = {The millions of pages of historical documents that are digitized in libraries are increasingly used in contexts that have more specific requirements for OCR quality than keyword search. How to comprehensively, efficiently and reliably assess the quality of OCR results against the background of mass digitization, when ground truth can only ever be produced for very small numbers? Due to gaps in specifications, results from OCR evaluation tools can return different results, and due to differences in implementation, even commonly used error rates are often not directly comparable. OCR evaluation metrics and sampling methods are also not sufficient where they do not take into account the accuracy of layout analysis, since for advanced use cases like Natural Language Processing or the Digital Humanities, accurate layout analysis and detection of the reading order are crucial. We provide an overview of OCR evaluation metrics and tools, describe two advanced use cases for OCR results, and perform an OCR evaluation experiment with multiple evaluation tools and different metrics for two distinct datasets. We analyze the differences and commonalities in light of the presented use cases and suggest areas for future work.},
   author = {Clemens Neudecker and Konstantin Baierer and Mike Gerber and Clausner Christian and Antonacopoulos Apostolos and Pletschacher Stefan},
   doi = {10.1145/3476887.3476888},
   isbn = {9781450386906},
   booktitle = {ACM International Conference Proceeding Series},
   keywords = {accuracy,evaluation,metrics,optical character recognition},
   month = {9},
   pages = {13-18},
   publisher = {Association for Computing Machinery},
   title = {A survey of OCR evaluation tools and metrics},
   year = {2021},
}
@misc{,
   abstract = {We present richer typesetting models that extend the unsupervised historical document recognition system of Berg-Kirkpatrick et al. (2013). The first model breaks the independence assumption between vertical offsets of neighboring glyphs and, in experiments, substantially decreases transcription error rates. The second model simultaneously learns multiple font styles and, as a result, is able to accurately track italic and non-italic portions of documents. Richer models complicate inference so we present a new, streamlined procedure that is over 25x faster than the method used by Berg-Kirkpatrick et al. (2013). Our final system achieves a relative word error reduction of 22% compared to state-of-the-art results on a dataset of historical newspapers .},
   author = {Taylor Berg-Kirkpatrick and Dan Klein},
   pages = {118-123},
   publisher = {Association for Computational Linguistics},
   title = {Improved Typesetting Models for Historical OCR},
   year = {2014},
}
@article{Li2021,
   abstract = {Text recognition is a long-standing research problem for document digitalization. Existing approaches are usually built based on CNN for image understanding and RNN for char-level text generation. In addition, another language model is usually needed to improve the overall accuracy as a post-processing step. In this paper, we propose an end-to-end text recognition approach with pre-trained image Transformer and text Transformer models, namely TrOCR, which leverages the Transformer architecture for both image understanding and wordpiece-level text generation. The TrOCR model is simple but effective, and can be pre-trained with large-scale synthetic data and fine-tuned with human-labeled datasets. Experiments show that the TrOCR model outperforms the current state-of-the-art models on the printed, handwritten and scene text recognition tasks. The TrOCR models and code are publicly available at \url\{https://aka.ms/trocr\}.},
   author = {Minghao Li and Tengchao Lv and Jingye Chen and Lei Cui and Yijuan Lu and Dinei Florencio and Cha Zhang and Zhoujun Li and Furu Wei},
   month = {9},
   title = {TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models},
   url = {http://arxiv.org/abs/2109.10282},
   year = {2021},
}
@article{,
   title = {takelab},
}
@misc{Garrette2015,
   abstract = {Transcribing documents from the printing press era, a challenge in its own right, is more complicated when documents interleave multiple languages-a common feature of 16th century texts. Additionally, many of these documents precede consistent ortho-graphic conventions, making the task even harder. We extend the state-of-the-art historical OCR model of Berg-Kirkpatrick et al. (2013) to handle word-level code-switching between multiple languages. Further, we enable our system to handle spelling variability , including now-obsolete shorthand systems used by printers. Our results show average relative character error reductions of 14% across a variety of historical texts.},
   author = {Dan Garrette and Hannah Alpert-Abrams and Taylor Berg-Kirkpatrick and Dan Klein},
   pages = {1036-1041},
   title = {Unsupervised Code-Switching for Multilingual Historical Document Transcription},
   year = {2015},
}
@article{,
   title = {Cagalj Comic-Book Reader},
}
@misc{,
   abstract = {Undergraduate thesis / Završni rad 2022 Degree Grantor / Ustanova koja je dodijelila akademski / stručni stupanj: University of Zagreb, Faculty of Humanities and Social Sciences / Sveučilište u Zagrebu, Filozofski fakultet Permanent link / Trajna poveznica: https://urn.nsk.hr/urn:nbn:hr:131:503631 Rights / Prava: In copyright / Zaštićeno autorskim pravom.},
   title = {Evaluacija OCR sustava Sumpor, Josipa},
   url = {https://urn.nsk.hr/urn:nbn:hr:131:503631},
}
@misc{Garrette2016,
   abstract = {Historical documents frequently exhibit extensive orthographic variation, including archaic spellings and obsolete shorthand. OCR tools typically seek to produce so-called diplomatic transcriptions that preserve these variants , but many end tasks require transcriptions with normalized orthography. In this paper, we present a novel joint transcription model that learns, unsupervised, a probabilistic mapping between modern orthography and that used in the document. Our system thus produces dual diplomatic and normalized transcriptions simultaneously, and achieves a 35% relative error reduction over a state-of-the-art OCR model on diplomatic transcription, and a 46% reduction on normalized transcription.},
   author = {Dan Garrette and Hannah Alpert-Abrams},
   title = {An Unsupervised Model of Orthographic Variation for Historical Document Transcription},
   year = {2016},
}
@article{Wick2018,
   abstract = {This paper proposes a combination of a convolutional and a LSTM network to improve the accuracy of OCR on early printed books. While the standard model of line based OCR uses a single LSTM layer, we utilize a CNN- and Pooling-Layer combination in advance of an LSTM layer. Due to the higher amount of trainable parameters the performance of the network relies on a high amount of training examples to unleash its power. Hereby, the error is reduced by a factor of up to 44%, yielding a CER of 1% and below. To further improve the results we use a voting mechanism to achieve character error rates (CER) below $0.5%$. The runtime of the deep model for training and prediction of a book behaves very similar to a shallow network.},
   author = {Christoph Wick and Christian Reul and Frank Puppe},
   month = {2},
   title = {Improving OCR Accuracy on Early Printed Books using Deep Convolutional Networks},
   url = {http://arxiv.org/abs/1802.10033},
   year = {2018},
}
@inproceedings{Springmann2014,
   abstract = {This paper deals with the application of OCR methods to historical printings of Latin texts. Whereas the problem of recognizing historical printings of modern languages has been the subject of the IMPACT program1, Latin has not yet been given any serious consideration despite the fact that it dominated literature production in Europe up to the 17th century. Using finite state tools and methods developed during the IMPACT program we show that efficent batch-oriented postcorrection can work for Latin as well, and that a lexicon of historical Latin spelling variants can be constructed to aid in the correction phase. Initial experiments for the OCR engines Tesseract and OCRopus show that some training on historical fonts and the application of lexical resources raise character accuracies beyond those of Finereader and that accuracies above 90% may be expected even for 16th century material.},
   author = {Uwe Springmann and Dietmar Najock and Hermann Morgenroth and Helmut Schmid and Annette Gotscharek and Florian Fink},
   doi = {10.1145/2595188.2595205},
   isbn = {9781450325882},
   booktitle = {ACM International Conference Proceeding Series},
   pages = {71-75},
   publisher = {Association for Computing Machinery},
   title = {OCR of historical printings of latin Texts: Problems, prospects, progress},
   year = {2014},
}
@misc{,
   title = {Information retrieval through optical character recognition},
   year = {2018},
}
@article{Zhang2022,
   abstract = {Digitization of scanned receipts aims to extract text from receipt images and save it into structured documents. This is usually split into two sub-tasks: text localization and optical character recognition (OCR). Most existing OCR models only focus on the cropped text instance images, which require the bounding box information provided by a text region detection model. Introducing an additional detector to identify the text instance images in advance adds complexity, however instance-level OCR models have very low accuracy when processing the whole image for the document-level OCR, such as receipt images containing multiple text lines arranged in various layouts. To this end, we propose a localization-free document-level OCR model for transcribing all the characters in a receipt image into an ordered sequence end-to-end. Specifically, we finetune the pretrained instance-level model TrOCR with randomly cropped image chunks, and gradually increase the image chunk size to generalize the recognition ability from instance images to full-page images. In our experiments on the SROIE receipt OCR dataset, the model finetuned with our strategy achieved 64.4 F1-score and a 22.8% character error rate (CER), respectively, which outperforms the baseline results with 48.5 F1-score and 50.6% CER. The best model, which splits the full image into 15 equally sized chunks, gives 87.8 F1-score and 4.98% CER with minimal additional pre or post-processing of the output. Moreover, the characters in the generated document-level sequences are arranged in the reading order, which is practical for real-world applications.},
   author = {Hongkuan Zhang and Edward Whittaker and Ikuo Kitagishi},
   month = {12},
   title = {Extending TrOCR for Text Localization-Free OCR of Full-Page Scanned Receipt Images},
   url = {http://arxiv.org/abs/2212.05525},
   year = {2022},
}
@misc{,
   abstract = {Master's thesis / Diplomski rad 2023 Degree Grantor / Ustanova koja je dodijelila akademski / stručni stupanj: University of Zagreb, Faculty of Humanities and Social Sciences / Sveučilište u Zagrebu, Filozofski fakultet Permanent link / Trajna poveznica: https://urn.nsk.hr/urn:nbn:hr:131:825916 Rights / Prava: In copyright / Zaštićeno autorskim pravom. Download date / Datum preuzimanja: 2024-02-29},
   author = {Stela Kmetič},
   title = {Digitalizacija i obrada slike i teksta u sustavima za optičko prepoznavanje znakova u domeni bibliotekarstva},
   url = {https://urn.nsk.hr/urn:nbn:hr:131:825916},
}
@misc{,
   author = {Martin Tomaschek},
   keywords = {benchmark,ocr},
   title = {Evaluation of off-the-shelf OCR technologies},
}
@article{Berg-Kirkpatrick2013,
   abstract = {We present a generative probabilistic model, inspired by historical printing processes , for transcribing images of documents from the printing press era. By jointly modeling the text of the document and the noisy (but regular) process of rendering glyphs, our unsupervised system is able to decipher font structure and more accurately transcribe images into text. Overall, our system substantially out-performs state-of-the-art solutions for this task, achieving a 31% relative reduction in word error rate over the leading commercial system for historical transcription, and a 47% relative reduction over Tesser-act, Google's open source OCR system.},
   author = {Taylor Berg-Kirkpatrick and Greg Durrett and Dan Klein},
   pages = {207-217},
   title = {Unsupervised Transcription of Historical Documents},
   url = {https://aclanthology.org/P13-1021/},
   year = {2013},
}
@misc{,
   author = {Sven Vidak Zagreb},
   title = {Jezično modeliranje hrvatskoga jezika modelima dubokoga učenja},
}
@inproceedings{Christy2017,
   abstract = {Optical character recognition (OCR) engineswork poorly on texts publishedwith premodern printing technologies. Engaging the key technological contributors fromthe IMPACT project, an earlier project attempting to solve the OCR problem for early modern and modern texts, the EarlyModernOCR Project (eMOP) of TexasA&Mreceived funding from theAndrewW.Mellon Foundation to improve OCR outputs for early modern texts from the Eighteenth Century Collections Online (ECCO) and Early English Books Online (EEBO) proprietary database products or some 45 million pages. Added to print problems are the poor quality of the page images in these collections, which would be too time consuming and expensive to reimage. This article describes eMOP's attempts to OCR 307,000 documents digitized from microfilm tomake our cultural heritage available for current and future researchers.We describe the reasoning behind our choices as we undertook the project based on other relevant studies; discoveries we made; the data and the system we developed for processing it; the software, algorithms, training procedures, and tools that we developed; and future directions that should be taken for further work in developing OCR engines for cultural heritage materials.&copy; 2017 ACM 1556-4673/2017/12-ART6 &dollar;15.00.},
   author = {Matthew Christy and Anshul Gupta and Elizabeth Grumbach and Laura Mandell and Richard Furuta and Ricardo Gutierrez-Osuna},
   doi = {10.1145/3075645},
   issn = {15564711},
   issue = {1},
   booktitle = {Journal on Computing and Cultural Heritage},
   keywords = {Digital humanities,Machine learning},
   month = {12},
   publisher = {Association for Computing Machinery},
   title = {Mass digitization of early modern textswith optical character recognition},
   volume = {11},
   year = {2017},
}
@misc{,
   abstract = {This article describes the results of a case study that applies Neural Network-based Optical Character Recognition (OCR) to scanned images of books printed between 1487 and 1870 by training the OCR engine OCRopus [Breuel et al. 2013] on the RIDGES herbal text corpus [Odebrecht et al. 2017] (in press). Training specific OCR models was possible because the necessary ground truth is available as error-corrected diplomatic transcriptions. The OCR results have been evaluated for accuracy against the ground truth of unseen test sets. Character and word accuracies (percentage of correctly recognized items) for the resulting machine-readable texts of individual documents range from 94% to more than 99% (character level) and from 76% to 97% (word level). This includes the earliest printed books, which were thought to be inaccessible by OCR methods until recently. Furthermore, OCR models trained on one part of the corpus consisting of books with different printing dates and different typesets (mixed models) have been tested for their predictive power on the books from the other part containing yet other fonts, mostly yielding character accuracies well above 90%. It therefore seems possible to construct generalized models trained on a range of fonts that can be applied to a wide variety of historical printings still giving good results. A moderate postcorrection effort of some pages will then enable the training of individual models with even better accuracies. Using this method, diachronic corpora including early printings can be constructed much faster and cheaper than by manual transcription. The OCR methods reported here open up the possibility of transforming our printed textual cultural heritage into electronic text by largely automatic means, which is a prerequisite for the mass conversion of scanned books.},
   author = {Anke Lüdeling <anke_Dot_Luedeling_At_Rz_Dot_Hu-Berlin_Dot_De>},
   title = {DHQ: Digital Humanities Quarterly OCR of historical printings with an application to building diachronic corpora: A case study using the RIDGES herbal corpus},
   year = {2017},
}
@misc{,
   author = {Rad Diplomski and Filipa Čagalj},
   title = {EKSTRAKCIJA TEKSTA IZ STRIPA ALAN FORD},
}
@article{,
   title = {Jurin-poster},
}
@misc{,
   abstract = {Master's thesis / Diplomski rad 2021 Degree Grantor / Ustanova koja je dodijelila akademski / stručni stupanj: University of Zadar / Sveučilište u Zadru Permanent link / Trajna poveznica: https://urn.nsk.hr/urn:nbn:hr:162:964713 Rights / Prava: In copyright / Zaštićeno autorskim pravom.},
   author = {Sunčana Jelovčić},
   title = {Usporedba aplikacija za optičko prepoznavanje znakova},
   url = {https://urn.nsk.hr/urn:nbn:hr:162:964713},
}
@misc{,
   abstract = {Latinčić, Ante Master's thesis / Diplomski rad 2020 Degree Grantor / Ustanova koja je dodijelila akademski / stručni stupanj: University of Split, Faculty of Maritime Studies / Sveučilište u Splitu, Pomorski fakultet Permanent link / Trajna poveznica: https://urn.nsk.hr/urn:nbn:hr:164:132525 Rights / Prava: In copyright / Zaštićeno autorskim pravom. Download date / Datum preuzimanja: 2023-05-09 Repository / Repozitorij: Repository-Faculty of Maritime Studies-Split-Repository-Faculty of Maritime Studies Split for permanent storage and preservation of digital resources of the institution},
   title = {Semantička segmentacija kretanja uporabom konvolucijskih neuronskih mreža},
   url = {https://urn.nsk.hr/urn:nbn:hr:164:132525},
}
@misc{,
   author = {Gidi Shperber and · Follow},
   title = {A gentle introduction to OCR},
   url = {www.Shibumi-ai.com},
}
@misc{,
   title = {MET, CONSECTETUER ADIPISCING ELIT, SED DIAM NONUMMY NIBH AOREET DOLORE MAGNA ALIQUAM ERAT VOLUTPAT. UT WISI ENIM NOSTRUD EXERCI TATION ULLAMCORPER SUSCIPIT LOBORTIS NISL MMODO CONSEQUAT. DUIS AUTEM VEL EUM IRIURE DOLOR IN VELIT ESSE MOLESTIE CONSEQUAT, VEL ILLUM DOLORE EU FEUGIAT O EROS ET ACCUMSAN ET IUSTO ODIO DIGNISSIM QUI BLANDIT ZRIL DELENIT AUGUE DUIS DOLORE TE FEUGAIT NULLA FACILISI},
}
@article{,
   title = {ImpactRepositoryPoster},
}
@article{,
   title = {Optical Character Recognition (Overview)},
}
@article{Zhao2023,
   abstract = {Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale language models. To discriminate the difference in parameter scale, the research community has coined the term large language models (LLM) for the PLMs of significant size. Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT, which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. In this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Besides, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions.},
   author = {Wayne Xin Zhao and Kun Zhou and Junyi Li and Tianyi Tang and Xiaolei Wang and Yupeng Hou and Yingqian Min and Beichen Zhang and Junjie Zhang and Zican Dong and Yifan Du and Chen Yang and Yushuo Chen and Zhipeng Chen and Jinhao Jiang and Ruiyang Ren and Yifan Li and Xinyu Tang and Zikang Liu and Peiyu Liu and Jian-Yun Nie and Ji-Rong Wen},
   month = {3},
   title = {A Survey of Large Language Models},
   url = {http://arxiv.org/abs/2303.18223},
   year = {2023},
}
@misc{,
   author = {Bc Pavel Andrlík Advisor and Ing Marek Hrúz Pilsen},
   title = {Optical character recognition using deep learning},
}
@article{,
   title = {Shrestha_Pramoj},
}
@misc{Rijhwani2022,
   author = {Shruti Rijhwani and Graham Neubig and Alan Black and Taylor Berg-Kirkpatrick and Daisy Rosenblum},
   title = {Improving Optical Character Recognition for Endangered Languages},
   year = {2022},
}
@misc{Li2022,
   author = {Chenxia Li and Weiwei Liu and Ruoyu Guo and Xiaoting Yin and Kaitao Jiang and Yongkun Du and Yuning Du and Lingfeng Zhu and Runjie Jin and Keying Liu and Yehua Yang and Ran Bi and Xiaoguang Hu and Dianhai Yu and Yanjun Ma},
   title = {Dive into OCR},
   year = {2022},
}
@article{,
   abstract = {In this paper, we present a model pretraining technique, named MaskOCR, for text recognition. Our text recognition architecture is an encoder-decoder transformer: the encoder extracts the patch-level representations, and the decoder recognizes the text from the representations. Our approach pretrains both the encoder and the decoder in a sequential manner. (i) We pretrain the encoder in a self-supervised manner over a large set of unlabeled real text images. We adopt the masked image modeling approach, which shows the effectiveness for general images, expecting that the representations take on semantics. (ii) We pretrain the decoder over a large set of synthesized text images in a supervised manner and enhance the language modeling capability of the decoder by randomly masking some text image patches occupied by characters input to the encoder and accordingly the representations input to the decoder. Experiments show that the proposed MaskOCR approach achieves superior results on the benchmark datasets, including Chinese and English text images.},
   author = {Pengyuan Lyu and Chengquan Zhang and Shanshan Liu and Meina Qiao and Yangliu Xu and Liang Wu and Kun Yao and Junyu Han and Errui Ding and Jingdong Wang},
   title = {MaskOCR: Text Recognition with Masked Encoder-Decoder Pretraining},
}
@article{Sporici2020,
   abstract = {Optical Character Recognition (OCR) is the process of identifying and converting texts rendered in images using pixels to a more computer-friendly representation. The presented work aims to prove that the accuracy of the Tesseract 4.0 OCR engine can be further enhanced by employing convolution-based preprocessing using specific kernels. As Tesseract 4.0 has proven great performance when evaluated against a favorable input, its capability of properly detecting and identifying characters in more realistic, unfriendly images is questioned. The article proposes an adaptive image preprocessing step guided by a reinforcement learning model, which attempts to minimize the edit distance between the recognized text and the ground truth. It is shown that this approach can boost the character-level accuracy of Tesseract 4.0 from 0.134 to 0.616 (+359% relative change) and the F1 score from 0.163 to 0.729 (+347% relative change) on a dataset that is considered challenging by its authors.},
   author = {Dan Sporici and Elena Cuşnir and Costin Anton Boiangiu},
   doi = {10.3390/SYM12050715},
   issn = {20738994},
   issue = {5},
   journal = {Symmetry},
   keywords = {Actor-critic model,Convolution,Convolutional neural network,Optical character recognition,Reinforcement learning,Tesseract,Unsupervised learning},
   month = {5},
   publisher = {MDPI AG},
   title = {Improving the accuracy of Tesseract 4.0 OCR engine using convolution-based preprocessing},
   volume = {12},
   year = {2020},
}
@inproceedings{Sabu2018,
   author = {Abin M Sabu and Anto Sahaya Das},
   doi = {10.1109/ICEDSS.2018.8544323},
   isbn = {978-1-5386-3479-0},
   booktitle = {2018 Conference on Emerging Devices and Smart Systems (ICEDSS)},
   month = {3},
   pages = {152-155},
   publisher = {IEEE},
   title = {A Survey on various Optical Character Recognition Techniques},
   url = {https://ieeexplore.ieee.org/document/8544323/},
   year = {2018},
}
@article{Borovikov2014,
   abstract = {This report explores the latest advances in the field of digital document recognition. With the focus on printed document imagery, we discuss the major developments in optical character recognition (OCR) and document image enhancement/restoration in application to Latin and non-Latin scripts. In addition, we review and discuss the available technologies for handwritten document recognition. In this report, we also provide some company-accumulated benchmark results on available OCR engines.},
   author = {Eugene Borovikov},
   keywords = {()},
   title = {A survey of modern optical character recognition techniques},
   year = {2014},
}
@article{Konanykhin2023,
   abstract = {The task of extracting information in the form of text from scanned or photographed objects using Optical Character Recognition (OCR) is an important and ubiquitous technology for digitizing and indexing physical documents. Existing technologies have shown to work correctly with near-ideal documents, but if the document is visually degraded or contains non-text elements, the quality of OCR can be greatly affected, especially due to false detections. In this article, we present an improved detection network with a masking system to improve the quality of document recognition. By filtering out non-text elements in an image, we can use document-level OCR to include contextual information to improve OCR results. We conduct a single evaluation of a publicly available data set, demonstrating the usefulness and broad applicability of our method. In addition, we present data and calculation results of a categorical cross entropy function specially tuned to improve data discovery results.},
   author = {Alexander Konanykhin and Tatyana Konanykhina and Vladimir Panishchev},
   doi = {10.1109/RUSAUTOCON58002.2023.10272925},
   isbn = {9798350345551},
   journal = {2023 International Russian Automation Conference (RusAutoCon)},
   keywords = {computer vision,high noise level,neural network,symbols,text recognition},
   pages = {930-935},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Character Recognition in Images under High Noise Levels},
   year = {2023},
}
@article{Shen2015,
   abstract = {one critical procedure in OCR is to detect text characters from a document image. However, some documents might come with embedded background images which often mislead the algorithms of character detection. For example, small dots or sharp edges from the background image are often bound-boxed as characters and passed to the next stage of the OCR pipeline, which causes an error chain. Motivated by this observation, we present a novel and cost-effective image preprocessing method to accomplish the task. We first enhance the document images before OCR by utilizing the brightness and chromaticity as contrast parameters. Then we convert color images to gray and threshold it. This way, background images can be removed effectively without losing the quality of text characters. The method was tested using Tesseract (an open source OCR engine) and compared with two commercial OCR software ABBYY Finereader and HANWANG (OCR software for Chinese characters). The experimental results show that the recognition accuracies are improved significantly after removing background images.},
   author = {Mande Shen and Hansheng Lei},
   doi = {10.1109/FSKD.2015.7382178},
   isbn = {9781467376822},
   journal = {International Conference on Fuzzy Systems and Knowledge Discovery},
   keywords = {OCR,background image,image preprocessing,recognition accuracy},
   month = {1},
   pages = {1566-1570},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Improving OCR performance with background image elimination},
   year = {2015},
}
@article{Randika2021,
   abstract = {Optical character recognition (OCR) is a widely used pattern recognition application in numerous domains. There are several feature-rich, general-purpose OCR solutions available for consumers, which can provide moderate to excellent accuracy levels. However, accuracy can diminish with difficult and uncommon document domains. Preprocessing of document images can be used to minimize the effect of domain shift. In this paper, a novel approach is presented for creating a customized preprocessor for a given OCR engine. Unlike the previous OCR agnostic preprocessing techniques, the proposed approach approximates the gradient of a particular OCR engine to train a preprocessor module. Experiments with two datasets and two OCR engines show that the presented preprocessor is able to improve the accuracy of the OCR up to 46% from the baseline by applying pixel-level manipulations to the document image. The implementation of the proposed method and the enhanced public datasets are available for download (https://github.com/paarandika/Gradient-Approx-to-improve-OCR ).},
   author = {Ayantha Randika and Nilanjan Ray and Xiao Xiao and Allegra Latimer},
   doi = {10.1007/978-3-030-86549-8_31},
   isbn = {9783030865481},
   issn = {16113349},
   journal = {IEEE International Conference on Document Analysis and Recognition},
   keywords = {Gradient approximation,OCR,Optical character recognition,Preprocessing},
   pages = {481-496},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Unknown-box Approximation to Improve Optical Character Recognition Performance},
   volume = {12821 LNCS},
   year = {2021},
}
@article{Vaswani2023,
   abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. * Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research. † Work performed while at Google Brain.},
   author = {Ashish Vaswani and Google Brain and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N Gomez and Łukasz Kaiser and Illia Polosukhin},
   isbn = {1706.03762v7},
   title = {Attention Is All You Need},
   year = {2023},
}
@article{Fujitake2023,
   abstract = {Typical text recognition methods rely on an encoder-decoder structure, in which the encoder extracts features from an image, and the decoder produces recognized text from these features. In this study, we propose a simpler and more effective method for text recognition, known as the Decoder-only Transformer for Optical Character Recognition (DTrOCR). This method uses a decoder-only Transformer to take advantage of a generative language model that is pre-trained on a large corpus. We examined whether a generative language model that has been successful in natural language processing can also be effective for text recognition in computer vision. Our experiments demonstrated that DTrOCR outperforms current state-of-the-art methods by a large margin in the recognition of printed, handwritten, and scene text in both English and Chinese.},
   author = {Masato Fujitake},
   month = {8},
   title = {DTrOCR: Decoder-only Transformer for Optical Character Recognition},
   url = {http://arxiv.org/abs/2308.15996},
   year = {2023},
}
@article{Olejniczak2022,
   abstract = {Detection and recognition of text from scans and other images, commonly denoted as Optical Character Recognition (OCR), is a widely used form of automated document processing with a number of methods available. Yet OCR systems still do not achieve 100% accuracy, requiring human corrections in applications where correct readout is essential. Advances in machine learning enabled even more challenging scenarios of text detection and recognition "in-the-wild" - such as detecting text on objects from photographs of complex scenes. While the state-of-the-art methods for in-the-wild text recognition are typically evaluated on complex scenes, their performance in the domain of documents is typically not published, and a comprehensive comparison with methods for document OCR is missing. This paper compares several methods designed for in-the-wild text recognition and for document text recognition, and provides their evaluation on the domain of structured documents. The results suggest that state-of-the-art methods originally proposed for in-the-wild text detection also achieve competitive results on document text detection, outperforming available OCR methods. We argue that the application of document OCR should not be omitted in evaluation of text detection and recognition methods.},
   author = {Krzysztof Olejniczak and Milan Šulc},
   month = {10},
   title = {Text Detection Forgot About Document OCR},
   year = {2022},
}
@article{Dhande2017,
   abstract = {This paper aims to represent the work related to recognition of cursive English handwriting. Character recognition of handwritten cursive English script is a very challenging task. In cursive English handwriting, the characters in a word are connected to each other. So the segmentation and feature extraction of cursive English script is much difficult. In the proposed work, horizontal and vertical projection methods are used for segmentation. Convex hull algorithm is used for feature extraction and SVM is used for classification and recognition.},
   author = {Pritam Dhande and Reena Kharat},
   doi = {10.1109/ICOEI.2017.8300915},
   isbn = {9781509042579},
   journal = {Proceedings - International Conference on Trends in Electronics and Informatics, ICEI 2017},
   keywords = {Feature extraction,Handwritten cursive English script,Optical character recognition,Segmentation},
   month = {7},
   pages = {199-203},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Recognition of cursive English handwritten characters},
   volume = {2018-January},
   year = {2017},
}
@article{Jyotsna2016,
   abstract = {Document Image binarization is the segmentation of the document into foreground text and background. It is done to obtain the clear images from which text can be retrieved easily. Thresholding is used for the segmentation of the document images. This paper, presents a review on various document image binarization techniques. Evaluation performance metrics used for the evaluation of the binarization techniques are also explained. Comparison of the performance of the binarization techniques based on the performance metrics like PSNR, F-Measure, NRM and MPM is shown. Performance of the techniques is evaluated on the dataset of DIBCO-2009 and DIBCO-2010.},
   author = {Jyotsna and Shivani Chauhan and Ekta Sharma and Amit Doegar},
   doi = {10.1109/ICRITO.2016.7784945},
   isbn = {9781509014897},
   journal = {2016 5th International Conference on Reliability, Infocom Technologies and Optimization, ICRITO 2016: Trends and Future Directions},
   keywords = {Thresholding,degraded document images,document image binarization,segmentation},
   month = {12},
   pages = {163-166},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Binarization techniques for degraded document images-A review},
   year = {2016},
}
@article{Otsu1979,
   abstract = {A nonparametric and unsupervised method of automatic threshold selection for picture segmentation is presented. An otpimal threshold is selected by the discriminant criterion, namely, so as the maximize the separability of the resultant classes in gray levels. The procedure is very simple, utilizing only the zeroth- and first-order cumulative moments of the gray-level histogram. It is strightforward to extend the method to multithreshold problems. Several experimental results are also presented to support the validity of the method.},
   author = {Nobuyuki Otsu},
   doi = {10.1109/TSMC.1979.4310076},
   issn = {00189472},
   issue = {1},
   journal = {IEEE Trans Syst Man Cybern},
   pages = {62-66},
   title = {THRESHOLD SELECTION METHOD FROM GRAY-LEVEL HISTOGRAMS.},
   volume = {SMC-9},
   year = {1979},
}
@article{Su2013,
   abstract = {Segmentation of text from badly degraded document images is a very challenging task due to the high inter/intra-variation between the document background and the foreground text of different document images. In this paper, we propose a novel document image binarization technique that addresses these issues by using adaptive image contrast. The adaptive image contrast is a combination of the local image contrast and the local image gradient that is tolerant to text and background variation caused by different types of document degradations. In the proposed technique, an adaptive contrast map is first constructed for an input degraded document image. The contrast map is then binarized and combined with Canny's edge map to identify the text stroke edge pixels. The document text is further segmented by a local threshold that is estimated based on the intensities of detected text stroke edge pixels within a local window. The proposed method is simple, robust, and involves minimum parameter tuning. It has been tested on three public datasets that are used in the recent document image binarization contest (DIBCO) 2009 & 2011 and handwritten-DIBCO 2010 and achieves accuracies of 93.5%, 87.8%, and 92.03%, respectively, that are significantly higher than or close to that of the best-performing methods reported in the three contests. Experiments on the Bickley diary dataset that consists of several challenging bad quality document images also show the superior performance of our proposed method, compared with other techniques. © 1992-2012 IEEE.},
   author = {Bolan Su and Shijian Lu and Chew Lim Tan},
   doi = {10.1109/TIP.2012.2231089},
   issn = {10577149},
   issue = {4},
   journal = {IEEE Transactions on Image Processing},
   keywords = {Adaptive image contrast,degraded document image binarization,document analysis,document image processing,pixel classification},
   pages = {1408-1417},
   title = {Robust document image binarization technique for degraded document images},
   volume = {22},
   url = {https://www.researchgate.net/publication/233879928_A_Robust_Document_Image_Binarization_Technique_for_Degraded_Document_Images},
   year = {2013},
}
@misc{,
   title = {End-to-end text recognition with convolutional neural networks | IEEE Conference Publication | IEEE Xplore},
   url = {https://ieeexplore.ieee.org/document/6460871},
}
@article{Breuel2013,
   abstract = {Long Short-Term Memory (LSTM) networks have yielded excellent results on handwriting recognition. This paper describes an application of bidirectional LSTM networks to the problem of machine-printed Latin and Fraktur recognition. Latin and Fraktur recognition differs significantly from handwriting recognition in both the statistical properties of the data, as well as in the required, much higher levels of accuracy. Applications of LSTM networks to handwriting recognition use two-dimensional recurrent networks, since the exact position and baseline of handwritten characters is variable. In contrast, for printed OCR, we used a one-dimensional recurrent network combined with a novel algorithm for baseline and x-height normalization. A number of databases were used for training and testing, including the UW3 database, artificially generated and degraded Fraktur text and scanned pages from a book digitization project. The LSTM architecture achieved 0.6% character-level test-set error on English text. When the artificially degraded Fraktur data set is divided into training and test sets, the system achieves an error rate of 1.64%. On specific books printed in Fraktur (not part of the training set), the system achieves error rates of 0.15% (Fontane) and 1.47% (Ersch-Gruber). These recognition accuracies were found without using any language modelling or any other post-processing techniques. © 2013 IEEE.},
   author = {Thomas M. Breuel and Adnan Ul-Hasan and Mayce Ali Al-Azawi and Faisal Shafait},
   doi = {10.1109/ICDAR.2013.140},
   issn = {15205363},
   journal = {Proceedings of the International Conference on Document Analysis and Recognition, ICDAR},
   keywords = {LSTM Networks,OCR,RNN},
   pages = {683-687},
   title = {High-performance OCR for printed english and fraktur using lstm networks},
   year = {2013},
}
@article{,
   title = {Tesseract Blends Old and New OCR Technology-DAS2016 Tutorial-Santorini-Greece Tesseract Blends Old and New OCR Technology-DAS2016 Tutorial-Santorini-Greece 3. Features and Character Classifier Background: Classical character classification},
}
@article{Brisinello2017,
   abstract = {Efficient Optical Character Recognition (OCR) in images grabbed from Set-Top Boxes (STBs) plays an important role in STB testing. However, running OCR software on such images usually ends with low OCR performance since images can have low resolution, low image quality or colorful background. In order to improve OCR performance, four different image preprocessing methods are proposed. In this paper OCR is performed with Tesseract 3.5 and the relatively new Tesseract 4.0 on the images grabbed from different STBs. On the original images Tesseract 3.5 provides a 35.7% accuracy while Tesseract 4.0 attains a 70.2% accuracy. The proposed preprocessing methods improve OCR performance by 33.3% for Tesseract 3.5 and 22.6% for Tesseract 4.0 on the available images.},
   author = {Matteo Brisinello and Ratko Grbic and Matija Pul and Tihomir Andelic},
   doi = {10.23919/ELMAR.2017.8124460},
   isbn = {9789531842303},
   issn = {13342630},
   journal = {Proceedings Elmar - International Symposium Electronics in Marine},
   keywords = {Image Preprocessing,Low Quality Images,OCR,Tesseract},
   month = {11},
   pages = {167-171},
   publisher = {Croatian Society Electronics in Marine - ELMAR},
   title = {Improving optical character recognition performance for low quality images},
   volume = {2017-September},
   year = {2017},
}
@misc{,
   title = {(PDF) VOTING-BASED OCR SYSTEM},
   url = {https://www.researchgate.net/publication/326016983_VOTING-BASED_OCR_SYSTEM},
}
@article{Kernycky2024,
   abstract = {In this paper we present the results of an evaluation study of the perfor-mance of LLMs on Technical Language Processing tasks. Humans are often confronted with tasks in which they have to gather information from dispar-ate sources and require making sense of large bodies of text. These tasks can be significantly complex for humans and often require deep study including rereading portions of a text. Towards simplifying the task of gathering in-formation we evaluated LLMs with chat interfaces for their ability to provide answers to standard questions that a human can be expected to answer based on their reading of a body of text. The body of text under study is Title 47 of the United States Code of Federal Regulations (CFR) which describes regula-tions for commercial telecommunications as governed by the Federal Com-munications Commission (FCC). This has been a body of text of interest be-cause our larger research concerns the issue of making sense of information related to Wireless Spectrum Governance and usage in an automated manner to support Dynamic Spectrum Access. The information concerning this wireless spectrum domain is found in many disparate sources, with Title 47 of the CFR being just one of many. Using a range of LLMs and providing the required CFR text as context we were able to quantify the performance of those LLMs on the specific task of answering the questions below.},
   author = {Andrew Kernycky and David Coleman and Christopher Spence and Udayan Das},
   doi = {10.1007/978-3-031-62110-9_8},
   keywords = {Generative AI,LLMs,Large Language Models,Wireless Spectrum},
   month = {3},
   pages = {75-85},
   title = {Evaluating the Performance of LLMs on Technical Language Processing tasks},
   url = {http://arxiv.org/abs/2403.15503 http://dx.doi.org/10.1007/978-3-031-62110-9_8},
   year = {2024},
}
@article{,
   abstract = {For more than half a century, the Hough transform is ever-expanding for new frontiers. Thousands of research papers and numerous applications have evolved over the decades. Carrying out an all-inclusive survey is hardly possible and enormously space-demanding. What we care about here is emphasizing some of the most crucial milestones of the transform. We describe its variations elaborating on the basic ones such as the line and circle Hough transforms. The high demand for storage and computation time is clarified with different solution approaches. Since most uses of the transform take place on binary images, we have been concerned with the work done directly on gray or color images. The myriad applications of the standard transform and its variations have been classified highlighting the up-to-date and the unconventional ones. Due to its merits such as noise-immunity and expandability, the transform has an excellent history, and a bright future as well.},
   author = {Allam Shehata Hassanein and Sherien Mohammad and Mohamed Sameer and Mohammad Ehab Ragab},
   keywords = {Color,Gray-Scale,Hough Transform,Memory Saving,Shapes,Speedup},
   title = {A Survey on Hough Transform, Theory, Techniques and Applications},
}
@article{Feser2021,
   abstract = {In his book ‘Is a Good God Logically Possible?’, James Sterba argues that the existence of much of the evil to be found in the world is logically incompatible with the existence of God. I defend the Thomistic view that when one properly understands the nature of God and of his relationship to the world, this so-called logical problem of evil does not arise. While Sterba has responded to the version of the Thomistic position presented by Brian Davies, I argue that his response fails.},
   author = {Edward Feser and James Sterba},
   doi = {10.3390/REL12040268},
   issn = {2077-1444},
   issue = {4},
   journal = {Religions 2021, Vol. 12, Page 268},
   keywords = {Brian Davies,James P. Sterba,Thomas Aquinas,problem of evil,theodicy},
   month = {4},
   pages = {268},
   publisher = {Multidisciplinary Digital Publishing Institute},
   title = {The Thomistic Dissolution of the Logical Problem of Evil},
   volume = {12},
   url = {https://www.mdpi.com/2077-1444/12/4/268/htm https://www.mdpi.com/2077-1444/12/4/268},
   year = {2021},
}
@misc{Leptonica,
   title = {Leptonica: Leptonica Reference Documentation},
   url = {https://tpgit.github.io/Leptonica/index.html},
}
@misc{Tesseract,
   title = {Tesseract Open Source OCR Engine (main repository)},
   url = {https://github.com/tesseract-ocr/tesseract/},
}
@misc{,
   abstract = {Recurrent Neural Networks have been my Achilles' heel for the past few months. Admittedly, I haven't had the grit to sit down and work out their details, but I've figured it's time I stop treating them like black boxes and try instead to discover what makes them tick. My intentions with this series are hence twofold: first, to combat my weakness by understanding their inner workings and coding one from scratch; and second, to write down what I learn in order to reinforce the insights I may gain along the way. In this first installment, we'll be introducing the intuition behind RNNs, motivating their use by highlighting a glaring limitation of traditional neural networks. We'll then transition into a more technical description of their architecture which will be useful for the next installment where we'll code one from scratch in numpy. We are the sum total of our experiences. None of us are the same as we were yesterday, nor will be tomorrow. B.J. Neblett There is an inherent truth to the quote above. Our brain pools from past experiences and combines them in intricate ways to solve new and unseen tasks. It is hardwired to work with sequences of information that we perpetually store and call upon over the course of our lives. At its core, human learning can be distilled into two fundamental processes: memorization: every time we gain new information, we store it for future reference. combination: not all tasks are the same, so we couple our analytical skills with a combination of our memorized, previous experiences to reason about the world. Consider the following pictures.},
   title = {Understanding Recurrent Neural Networks-Part I Human Learning},
   year = {2017},
}
@article{Sauvola1997,
   abstract = {A new method is presented for adaptive document image binarization, where the page is considered as a collection of subcomponents such as text, background and picture. The problems caused by noise, illumination and many source type related degradations are addressed. The algorithm uses document characteristics to determine (surface) attributes, often used in document segmentation. Using characteristics analysis, two new algorithms are applied to determine a local threshold for each pixel. An algorithm based on soft decision control is used for thresholding background and picture regions. An approach utilizing local mean and variance of gray values is applied to textual regions. Tests were performed with images including different types of document components and degradations. The results show that the method adapts and performs well in each case.},
   author = {Jaakko Sauvola and Tapio Seppanen and Sami Haapakoski and Matti Pietikainen},
   doi = {10.1109/ICDAR.1997.619831},
   journal = {Proceedings of the International Conference on Document Analysis and Recognition, ICDAR},
   pages = {147-152},
   publisher = {IEEE},
   title = {Adaptive document binarization},
   volume = {1},
   year = {1997},
}
