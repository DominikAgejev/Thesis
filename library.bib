@inproceedings{Smith2007,
   abstract = {The Tesseract OCR engine, as was the HP Research Prototype in the UNLV Fourth Annual Test of OCR Accuracy[1], is described in a comprehensive overview. Emphasis is placed on aspects that are novel or at least unusual in an OCR engine, including in particular the line finding, features/classification methods, and the adaptive classifier.},
   author = {R. Smith},
   doi = {10.1109/ICDAR.2007.4376991},
   isbn = {0-7695-2822-8},
   issn = {1520-5363},
   booktitle = {Ninth International Conference on Document Analysis and Recognition (ICDAR 2007) Vol 2},
   month = {9},
   pages = {629-633},
   publisher = {IEEE},
   title = {An Overview of the Tesseract OCR Engine},
   url = {http://ieeexplore.ieee.org/document/4376991/},
   year = {2007},
}
@inproceedings{Neudecker2021,
   abstract = {The millions of pages of historical documents that are digitized in libraries are increasingly used in contexts that have more specific requirements for OCR quality than keyword search. How to comprehensively, efficiently and reliably assess the quality of OCR results against the background of mass digitization, when ground truth can only ever be produced for very small numbers? Due to gaps in specifications, results from OCR evaluation tools can return different results, and due to differences in implementation, even commonly used error rates are often not directly comparable. OCR evaluation metrics and sampling methods are also not sufficient where they do not take into account the accuracy of layout analysis, since for advanced use cases like Natural Language Processing or the Digital Humanities, accurate layout analysis and detection of the reading order are crucial. We provide an overview of OCR evaluation metrics and tools, describe two advanced use cases for OCR results, and perform an OCR evaluation experiment with multiple evaluation tools and different metrics for two distinct datasets. We analyze the differences and commonalities in light of the presented use cases and suggest areas for future work.},
   author = {Clemens Neudecker and Konstantin Baierer and Mike Gerber and Clausner Christian and Antonacopoulos Apostolos and Pletschacher Stefan},
   doi = {10.1145/3476887.3476888},
   isbn = {9781450386906},
   booktitle = {ACM International Conference Proceeding Series},
   keywords = {accuracy,evaluation,metrics,optical character recognition},
   month = {9},
   pages = {13-18},
   publisher = {Association for Computing Machinery},
   title = {A survey of OCR evaluation tools and metrics},
   year = {2021},
}
@misc{Berg2014,
   abstract = {We present richer typesetting models that extend the unsupervised historical document recognition system of Berg-Kirkpatrick et al. (2013). The first model breaks the independence assumption between vertical offsets of neighboring glyphs and, in experiments, substantially decreases transcription error rates. The second model simultaneously learns multiple font styles and, as a result, is able to accurately track italic and non-italic portions of documents. Richer models complicate inference so we present a new, streamlined procedure that is over 25x faster than the method used by Berg-Kirkpatrick et al. (2013). Our final system achieves a relative word error reduction of 22% compared to state-of-the-art results on a dataset of historical newspapers .},
   author = {Taylor Berg-Kirkpatrick and Dan Klein},
   doi = {10.3115/v1/P14-2020},
   pages = {118-123},
   publisher = {Association for Computational Linguistics},
   title = {Improved Typesetting Models for Historical OCR},
   year = {2014},
}
@article{Li2021,
   abstract = {Text recognition is a long-standing research problem for document digitalization. Existing approaches are usually built based on CNN for image understanding and RNN for char-level text generation. In addition, another language model is usually needed to improve the overall accuracy as a post-processing step. In this paper, we propose an end-to-end text recognition approach with pre-trained image Transformer and text Transformer models, namely TrOCR, which leverages the Transformer architecture for both image understanding and wordpiece-level text generation. The TrOCR model is simple but effective, and can be pre-trained with large-scale synthetic data and fine-tuned with human-labeled datasets. Experiments show that the TrOCR model outperforms the current state-of-the-art models on the printed, handwritten and scene text recognition tasks. The TrOCR models and code are publicly available at \url\{https://aka.ms/trocr\}.},
   author = {Minghao Li and Tengchao Lv and Jingye Chen and Lei Cui and Yijuan Lu and Dinei Florencio and Cha Zhang and Zhoujun Li and Furu Wei},
   month = {9},
   title = {TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models},
   url = {http://arxiv.org/abs/2109.10282},
   year = {2021},
}
@misc{Garrette2015,
   abstract = {Transcribing documents from the printing press era, a challenge in its own right, is more complicated when documents interleave multiple languages-a common feature of 16th century texts. Additionally, many of these documents precede consistent ortho-graphic conventions, making the task even harder. We extend the state-of-the-art historical OCR model of Berg-Kirkpatrick et al. (2013) to handle word-level code-switching between multiple languages. Further, we enable our system to handle spelling variability , including now-obsolete shorthand systems used by printers. Our results show average relative character error reductions of 14% across a variety of historical texts.},
   author = {Dan Garrette and Hannah Alpert-Abrams and Taylor Berg-Kirkpatrick and Dan Klein},
   pages = {1036-1041},
   title = {Unsupervised Code-Switching for Multilingual Historical Document Transcription},
   year = {2015},
}
@misc{Garrette2016,
   abstract = {Historical documents frequently exhibit extensive orthographic variation, including archaic spellings and obsolete shorthand. OCR tools typically seek to produce so-called diplomatic transcriptions that preserve these variants , but many end tasks require transcriptions with normalized orthography. In this paper, we present a novel joint transcription model that learns, unsupervised, a probabilistic mapping between modern orthography and that used in the document. Our system thus produces dual diplomatic and normalized transcriptions simultaneously, and achieves a 35% relative error reduction over a state-of-the-art OCR model on diplomatic transcription, and a 46% reduction on normalized transcription.},
   author = {Dan Garrette and Hannah Alpert-Abrams},
   title = {An Unsupervised Model of Orthographic Variation for Historical Document Transcription},
   year = {2016},
}
@article{Wick2018,
   abstract = {This paper proposes a combination of a convolutional and a LSTM network to improve the accuracy of OCR on early printed books. While the standard model of line based OCR uses a single LSTM layer, we utilize a CNN- and Pooling-Layer combination in advance of an LSTM layer. Due to the higher amount of trainable parameters the performance of the network relies on a high amount of training examples to unleash its power. Hereby, the error is reduced by a factor of up to 44%, yielding a CER of 1% and below. To further improve the results we use a voting mechanism to achieve character error rates (CER) below $0.5%$. The runtime of the deep model for training and prediction of a book behaves very similar to a shallow network.},
   author = {Christoph Wick and Christian Reul and Frank Puppe},
   month = {2},
   title = {Improving OCR Accuracy on Early Printed Books using Deep Convolutional Networks},
   url = {http://arxiv.org/abs/1802.10033},
   year = {2018},
}
@inproceedings{Springmann2014,
   abstract = {This paper deals with the application of OCR methods to historical printings of Latin texts. Whereas the problem of recognizing historical printings of modern languages has been the subject of the IMPACT program1, Latin has not yet been given any serious consideration despite the fact that it dominated literature production in Europe up to the 17th century. Using finite state tools and methods developed during the IMPACT program we show that efficent batch-oriented postcorrection can work for Latin as well, and that a lexicon of historical Latin spelling variants can be constructed to aid in the correction phase. Initial experiments for the OCR engines Tesseract and OCRopus show that some training on historical fonts and the application of lexical resources raise character accuracies beyond those of Finereader and that accuracies above 90% may be expected even for 16th century material.},
   author = {Uwe Springmann and Dietmar Najock and Hermann Morgenroth and Helmut Schmid and Annette Gotscharek and Florian Fink},
   doi = {10.1145/2595188.2595205},
   isbn = {9781450325882},
   booktitle = {ACM International Conference Proceeding Series},
   pages = {71-75},
   publisher = {Association for Computing Machinery},
   title = {OCR of historical printings of latin Texts: Problems, prospects, progress},
   year = {2014},
}
@article{Zhang2022,
   abstract = {Digitization of scanned receipts aims to extract text from receipt images and save it into structured documents. This is usually split into two sub-tasks: text localization and optical character recognition (OCR). Most existing OCR models only focus on the cropped text instance images, which require the bounding box information provided by a text region detection model. Introducing an additional detector to identify the text instance images in advance adds complexity, however instance-level OCR models have very low accuracy when processing the whole image for the document-level OCR, such as receipt images containing multiple text lines arranged in various layouts. To this end, we propose a localization-free document-level OCR model for transcribing all the characters in a receipt image into an ordered sequence end-to-end. Specifically, we finetune the pretrained instance-level model TrOCR with randomly cropped image chunks, and gradually increase the image chunk size to generalize the recognition ability from instance images to full-page images. In our experiments on the SROIE receipt OCR dataset, the model finetuned with our strategy achieved 64.4 F1-score and a 22.8% character error rate (CER), respectively, which outperforms the baseline results with 48.5 F1-score and 50.6% CER. The best model, which splits the full image into 15 equally sized chunks, gives 87.8 F1-score and 4.98% CER with minimal additional pre or post-processing of the output. Moreover, the characters in the generated document-level sequences are arranged in the reading order, which is practical for real-world applications.},
   author = {Hongkuan Zhang and Edward Whittaker and Ikuo Kitagishi},
   month = {12},
   title = {Extending TrOCR for Text Localization-Free OCR of Full-Page Scanned Receipt Images},
   url = {http://arxiv.org/abs/2212.05525},
   year = {2022},
}
@misc{,
   author = {Martin Tomaschek},
   keywords = {benchmark,ocr},
   title = {Evaluation of off-the-shelf OCR technologies},
}
@misc{Berg2013,
   abstract = {We present a generative probabilistic model, inspired by historical printing processes , for transcribing images of documents from the printing press era. By jointly modeling the text of the document and the noisy (but regular) process of rendering glyphs, our unsupervised system is able to decipher font structure and more accurately transcribe images into text. Overall, our system substantially out-performs state-of-the-art solutions for this task, achieving a 31% relative reduction in word error rate over the leading commercial system for historical transcription, and a 47% relative reduction over Tesser-act, Google's open source OCR system.},
   author = {Taylor Berg-Kirkpatrick and Greg Durrett and Dan Klein},
   pages = {207-217},
   title = {Unsupervised Transcription of Historical Documents},
   url = {https://aclanthology.org/P13-1021/},
   year = {2013},
}
@inproceedings{Christy2017,
   abstract = {Optical character recognition (OCR) engineswork poorly on texts publishedwith premodern printing technologies. Engaging the key technological contributors fromthe IMPACT project, an earlier project attempting to solve the OCR problem for early modern and modern texts, the EarlyModernOCR Project (eMOP) of TexasA&Mreceived funding from theAndrewW.Mellon Foundation to improve OCR outputs for early modern texts from the Eighteenth Century Collections Online (ECCO) and Early English Books Online (EEBO) proprietary database products or some 45 million pages. Added to print problems are the poor quality of the page images in these collections, which would be too time consuming and expensive to reimage. This article describes eMOP's attempts to OCR 307,000 documents digitized from microfilm tomake our cultural heritage available for current and future researchers.We describe the reasoning behind our choices as we undertook the project based on other relevant studies; discoveries we made; the data and the system we developed for processing it; the software, algorithms, training procedures, and tools that we developed; and future directions that should be taken for further work in developing OCR engines for cultural heritage materials.&copy; 2017 ACM 1556-4673/2017/12-ART6 &dollar;15.00.},
   author = {Matthew Christy and Anshul Gupta and Elizabeth Grumbach and Laura Mandell and Richard Furuta and Ricardo Gutierrez-Osuna},
   doi = {10.1145/3075645},
   issn = {15564711},
   issue = {1},
   booktitle = {Journal on Computing and Cultural Heritage},
   keywords = {Digital humanities,Machine learning},
   month = {12},
   publisher = {Association for Computing Machinery},
   title = {Mass digitization of early modern textswith optical character recognition},
   volume = {11},
   year = {2017},
}
@misc{,
   abstract = {This article describes the results of a case study that applies Neural Network-based Optical Character Recognition (OCR) to scanned images of books printed between 1487 and 1870 by training the OCR engine OCRopus [Breuel et al. 2013] on the RIDGES herbal text corpus [Odebrecht et al. 2017] (in press). Training specific OCR models was possible because the necessary ground truth is available as error-corrected diplomatic transcriptions. The OCR results have been evaluated for accuracy against the ground truth of unseen test sets. Character and word accuracies (percentage of correctly recognized items) for the resulting machine-readable texts of individual documents range from 94% to more than 99% (character level) and from 76% to 97% (word level). This includes the earliest printed books, which were thought to be inaccessible by OCR methods until recently. Furthermore, OCR models trained on one part of the corpus consisting of books with different printing dates and different typesets (mixed models) have been tested for their predictive power on the books from the other part containing yet other fonts, mostly yielding character accuracies well above 90%. It therefore seems possible to construct generalized models trained on a range of fonts that can be applied to a wide variety of historical printings still giving good results. A moderate postcorrection effort of some pages will then enable the training of individual models with even better accuracies. Using this method, diachronic corpora including early printings can be constructed much faster and cheaper than by manual transcription. The OCR methods reported here open up the possibility of transforming our printed textual cultural heritage into electronic text by largely automatic means, which is a prerequisite for the mass conversion of scanned books.},
   author = {Anke Lüdeling <anke_Dot_Luedeling_At_Rz_Dot_Hu-Berlin_Dot_De>},
   title = {DHQ: Digital Humanities Quarterly OCR of historical printings with an application to building diachronic corpora: A case study using the RIDGES herbal corpus},
   year = {2017},
}
@misc{,
   author = {Rad Diplomski and Filipa Čagalj},
   title = {EKSTRAKCIJA TEKSTA IZ STRIPA ALAN FORD},
}
@article{Zhao2023,
   abstract = {Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale language models. To discriminate the difference in parameter scale, the research community has coined the term large language models (LLM) for the PLMs of significant size. Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT, which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. In this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Besides, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions.},
   author = {Wayne Xin Zhao and Kun Zhou and Junyi Li and Tianyi Tang and Xiaolei Wang and Yupeng Hou and Yingqian Min and Beichen Zhang and Junjie Zhang and Zican Dong and Yifan Du and Chen Yang and Yushuo Chen and Zhipeng Chen and Jinhao Jiang and Ruiyang Ren and Yifan Li and Xinyu Tang and Zikang Liu and Peiyu Liu and Jian-Yun Nie and Ji-Rong Wen},
   month = {3},
   title = {A Survey of Large Language Models},
   url = {http://arxiv.org/abs/2303.18223},
   year = {2023},
}
@misc{,
   author = {Bc Pavel Andrlík Advisor and Ing Marek Hrúz Pilsen},
   title = {Optical character recognition using deep learning},
}
@misc{Rijhwani2022,
   author = {Shruti Rijhwani and Graham Neubig and Alan Black and Taylor Berg-Kirkpatrick and Daisy Rosenblum},
   title = {Improving Optical Character Recognition for Endangered Languages},
   year = {2022},
}
@misc{Li2022,
   author = {Chenxia Li and Weiwei Liu and Ruoyu Guo and Xiaoting Yin and Kaitao Jiang and Yongkun Du and Yuning Du and Lingfeng Zhu and Runjie Jin and Keying Liu and Yehua Yang and Ran Bi and Xiaoguang Hu and Dianhai Yu and Yanjun Ma},
   title = {Dive into OCR},
   year = {2022},
}
@article{,
   abstract = {In this paper, we present a model pretraining technique, named MaskOCR, for text recognition. Our text recognition architecture is an encoder-decoder transformer: the encoder extracts the patch-level representations, and the decoder recognizes the text from the representations. Our approach pretrains both the encoder and the decoder in a sequential manner. (i) We pretrain the encoder in a self-supervised manner over a large set of unlabeled real text images. We adopt the masked image modeling approach, which shows the effectiveness for general images, expecting that the representations take on semantics. (ii) We pretrain the decoder over a large set of synthesized text images in a supervised manner and enhance the language modeling capability of the decoder by randomly masking some text image patches occupied by characters input to the encoder and accordingly the representations input to the decoder. Experiments show that the proposed MaskOCR approach achieves superior results on the benchmark datasets, including Chinese and English text images.},
   author = {Pengyuan Lyu and Chengquan Zhang and Shanshan Liu and Meina Qiao and Yangliu Xu and Liang Wu and Kun Yao and Junyu Han and Errui Ding and Jingdong Wang},
   title = {MaskOCR: Text Recognition with Masked Encoder-Decoder Pretraining},
}
@article{Sporici2020,
   abstract = {Optical Character Recognition (OCR) is the process of identifying and converting texts rendered in images using pixels to a more computer-friendly representation. The presented work aims to prove that the accuracy of the Tesseract 4.0 OCR engine can be further enhanced by employing convolution-based preprocessing using specific kernels. As Tesseract 4.0 has proven great performance when evaluated against a favorable input, its capability of properly detecting and identifying characters in more realistic, unfriendly images is questioned. The article proposes an adaptive image preprocessing step guided by a reinforcement learning model, which attempts to minimize the edit distance between the recognized text and the ground truth. It is shown that this approach can boost the character-level accuracy of Tesseract 4.0 from 0.134 to 0.616 (+359% relative change) and the F1 score from 0.163 to 0.729 (+347% relative change) on a dataset that is considered challenging by its authors.},
   author = {Dan Sporici and Elena Cuşnir and Costin Anton Boiangiu},
   doi = {10.3390/SYM12050715},
   issn = {20738994},
   issue = {5},
   journal = {Symmetry},
   keywords = {Actor-critic model,Convolution,Convolutional neural network,Optical character recognition,Reinforcement learning,Tesseract,Unsupervised learning},
   month = {5},
   publisher = {MDPI AG},
   title = {Improving the accuracy of Tesseract 4.0 OCR engine using convolution-based preprocessing},
   volume = {12},
   year = {2020},
}
@inproceedings{Sabu2018,
   author = {Abin M Sabu and Anto Sahaya Das},
   doi = {10.1109/ICEDSS.2018.8544323},
   isbn = {978-1-5386-3479-0},
   booktitle = {2018 Conference on Emerging Devices and Smart Systems (ICEDSS)},
   month = {3},
   pages = {152-155},
   publisher = {IEEE},
   title = {A Survey on various Optical Character Recognition Techniques},
   url = {https://ieeexplore.ieee.org/document/8544323/},
   year = {2018},
}
@article{Borovikov2014,
   abstract = {This report explores the latest advances in the field of digital document recognition. With the focus on printed document imagery, we discuss the major developments in optical character recognition (OCR) and document image enhancement/restoration in application to Latin and non-Latin scripts. In addition, we review and discuss the available technologies for handwritten document recognition. In this report, we also provide some company-accumulated benchmark results on available OCR engines.},
   author = {Eugene Borovikov},
   keywords = {()},
   title = {A survey of modern optical character recognition techniques},
   year = {2014},
}
@article{Konanykhin2023,
   abstract = {The task of extracting information in the form of text from scanned or photographed objects using Optical Character Recognition (OCR) is an important and ubiquitous technology for digitizing and indexing physical documents. Existing technologies have shown to work correctly with near-ideal documents, but if the document is visually degraded or contains non-text elements, the quality of OCR can be greatly affected, especially due to false detections. In this article, we present an improved detection network with a masking system to improve the quality of document recognition. By filtering out non-text elements in an image, we can use document-level OCR to include contextual information to improve OCR results. We conduct a single evaluation of a publicly available data set, demonstrating the usefulness and broad applicability of our method. In addition, we present data and calculation results of a categorical cross entropy function specially tuned to improve data discovery results.},
   author = {Alexander Konanykhin and Tatyana Konanykhina and Vladimir Panishchev},
   doi = {10.1109/RUSAUTOCON58002.2023.10272925},
   isbn = {9798350345551},
   journal = {2023 International Russian Automation Conference (RusAutoCon)},
   keywords = {computer vision,high noise level,neural network,symbols,text recognition},
   pages = {930-935},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Character Recognition in Images under High Noise Levels},
   year = {2023},
}
@article{Shen2015,
   abstract = {one critical procedure in OCR is to detect text characters from a document image. However, some documents might come with embedded background images which often mislead the algorithms of character detection. For example, small dots or sharp edges from the background image are often bound-boxed as characters and passed to the next stage of the OCR pipeline, which causes an error chain. Motivated by this observation, we present a novel and cost-effective image preprocessing method to accomplish the task. We first enhance the document images before OCR by utilizing the brightness and chromaticity as contrast parameters. Then we convert color images to gray and threshold it. This way, background images can be removed effectively without losing the quality of text characters. The method was tested using Tesseract (an open source OCR engine) and compared with two commercial OCR software ABBYY Finereader and HANWANG (OCR software for Chinese characters). The experimental results show that the recognition accuracies are improved significantly after removing background images.},
   author = {Mande Shen and Hansheng Lei},
   doi = {10.1109/FSKD.2015.7382178},
   isbn = {9781467376822},
   journal = {International Conference on Fuzzy Systems and Knowledge Discovery},
   keywords = {OCR,background image,image preprocessing,recognition accuracy},
   month = {1},
   pages = {1566-1570},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Improving OCR performance with background image elimination},
   year = {2015},
}
@article{Randika2021,
   abstract = {Optical character recognition (OCR) is a widely used pattern recognition application in numerous domains. There are several feature-rich, general-purpose OCR solutions available for consumers, which can provide moderate to excellent accuracy levels. However, accuracy can diminish with difficult and uncommon document domains. Preprocessing of document images can be used to minimize the effect of domain shift. In this paper, a novel approach is presented for creating a customized preprocessor for a given OCR engine. Unlike the previous OCR agnostic preprocessing techniques, the proposed approach approximates the gradient of a particular OCR engine to train a preprocessor module. Experiments with two datasets and two OCR engines show that the presented preprocessor is able to improve the accuracy of the OCR up to 46% from the baseline by applying pixel-level manipulations to the document image. The implementation of the proposed method and the enhanced public datasets are available for download (https://github.com/paarandika/Gradient-Approx-to-improve-OCR ).},
   author = {Ayantha Randika and Nilanjan Ray and Xiao Xiao and Allegra Latimer},
   doi = {10.1007/978-3-030-86549-8_31},
   isbn = {9783030865481},
   issn = {16113349},
   journal = {IEEE International Conference on Document Analysis and Recognition},
   keywords = {Gradient approximation,OCR,Optical character recognition,Preprocessing},
   pages = {481-496},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Unknown-box Approximation to Improve Optical Character Recognition Performance},
   volume = {12821 LNCS},
   year = {2021},
}
@article{Vaswani2023,
   abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. * Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research. † Work performed while at Google Brain.},
   author = {Ashish Vaswani and Google Brain and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N Gomez and Łukasz Kaiser and Illia Polosukhin},
   isbn = {1706.03762v7},
   title = {Attention Is All You Need},
   year = {2023},
}
@article{Fujitake2023,
   abstract = {Typical text recognition methods rely on an encoder-decoder structure, in which the encoder extracts features from an image, and the decoder produces recognized text from these features. In this study, we propose a simpler and more effective method for text recognition, known as the Decoder-only Transformer for Optical Character Recognition (DTrOCR). This method uses a decoder-only Transformer to take advantage of a generative language model that is pre-trained on a large corpus. We examined whether a generative language model that has been successful in natural language processing can also be effective for text recognition in computer vision. Our experiments demonstrated that DTrOCR outperforms current state-of-the-art methods by a large margin in the recognition of printed, handwritten, and scene text in both English and Chinese.},
   author = {Masato Fujitake},
   month = {8},
   title = {DTrOCR: Decoder-only Transformer for Optical Character Recognition},
   url = {http://arxiv.org/abs/2308.15996},
   year = {2023},
}
@article{Olejniczak2022,
   abstract = {Detection and recognition of text from scans and other images, commonly denoted as Optical Character Recognition (OCR), is a widely used form of automated document processing with a number of methods available. Yet OCR systems still do not achieve 100% accuracy, requiring human corrections in applications where correct readout is essential. Advances in machine learning enabled even more challenging scenarios of text detection and recognition "in-the-wild" - such as detecting text on objects from photographs of complex scenes. While the state-of-the-art methods for in-the-wild text recognition are typically evaluated on complex scenes, their performance in the domain of documents is typically not published, and a comprehensive comparison with methods for document OCR is missing. This paper compares several methods designed for in-the-wild text recognition and for document text recognition, and provides their evaluation on the domain of structured documents. The results suggest that state-of-the-art methods originally proposed for in-the-wild text detection also achieve competitive results on document text detection, outperforming available OCR methods. We argue that the application of document OCR should not be omitted in evaluation of text detection and recognition methods.},
   author = {Krzysztof Olejniczak and Milan Šulc},
   month = {10},
   title = {Text Detection Forgot About Document OCR},
   year = {2022},
}
@article{Dhande2017,
   abstract = {This paper aims to represent the work related to recognition of cursive English handwriting. Character recognition of handwritten cursive English script is a very challenging task. In cursive English handwriting, the characters in a word are connected to each other. So the segmentation and feature extraction of cursive English script is much difficult. In the proposed work, horizontal and vertical projection methods are used for segmentation. Convex hull algorithm is used for feature extraction and SVM is used for classification and recognition.},
   author = {Pritam Dhande and Reena Kharat},
   doi = {10.1109/ICOEI.2017.8300915},
   isbn = {9781509042579},
   journal = {Proceedings - International Conference on Trends in Electronics and Informatics, ICEI 2017},
   keywords = {Feature extraction,Handwritten cursive English script,Optical character recognition,Segmentation},
   month = {7},
   pages = {199-203},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Recognition of cursive English handwritten characters},
   volume = {2018-January},
   year = {2017},
}
@article{Jyotsna2016,
   abstract = {Document Image binarization is the segmentation of the document into foreground text and background. It is done to obtain the clear images from which text can be retrieved easily. Thresholding is used for the segmentation of the document images. This paper, presents a review on various document image binarization techniques. Evaluation performance metrics used for the evaluation of the binarization techniques are also explained. Comparison of the performance of the binarization techniques based on the performance metrics like PSNR, F-Measure, NRM and MPM is shown. Performance of the techniques is evaluated on the dataset of DIBCO-2009 and DIBCO-2010.},
   author = {Jyotsna and Shivani Chauhan and Ekta Sharma and Amit Doegar},
   doi = {10.1109/ICRITO.2016.7784945},
   isbn = {9781509014897},
   journal = {2016 5th International Conference on Reliability, Infocom Technologies and Optimization, ICRITO 2016: Trends and Future Directions},
   keywords = {Thresholding,degraded document images,document image binarization,segmentation},
   month = {12},
   pages = {163-166},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Binarization techniques for degraded document images-A review},
   year = {2016},
}
@article{Otsu1979,
   abstract = {A nonparametric and unsupervised method of automatic threshold selection for picture segmentation is presented. An otpimal threshold is selected by the discriminant criterion, namely, so as the maximize the separability of the resultant classes in gray levels. The procedure is very simple, utilizing only the zeroth- and first-order cumulative moments of the gray-level histogram. It is strightforward to extend the method to multithreshold problems. Several experimental results are also presented to support the validity of the method.},
   author = {Nobuyuki Otsu},
   doi = {10.1109/TSMC.1979.4310076},
   issn = {00189472},
   issue = {1},
   journal = {IEEE Trans Syst Man Cybern},
   pages = {62-66},
   title = {THRESHOLD SELECTION METHOD FROM GRAY-LEVEL HISTOGRAMS.},
   volume = {SMC-9},
   year = {1979},
}
@article{Su2013,
   abstract = {Segmentation of text from badly degraded document images is a very challenging task due to the high inter/intra-variation between the document background and the foreground text of different document images. In this paper, we propose a novel document image binarization technique that addresses these issues by using adaptive image contrast. The adaptive image contrast is a combination of the local image contrast and the local image gradient that is tolerant to text and background variation caused by different types of document degradations. In the proposed technique, an adaptive contrast map is first constructed for an input degraded document image. The contrast map is then binarized and combined with Canny's edge map to identify the text stroke edge pixels. The document text is further segmented by a local threshold that is estimated based on the intensities of detected text stroke edge pixels within a local window. The proposed method is simple, robust, and involves minimum parameter tuning. It has been tested on three public datasets that are used in the recent document image binarization contest (DIBCO) 2009 & 2011 and handwritten-DIBCO 2010 and achieves accuracies of 93.5%, 87.8%, and 92.03%, respectively, that are significantly higher than or close to that of the best-performing methods reported in the three contests. Experiments on the Bickley diary dataset that consists of several challenging bad quality document images also show the superior performance of our proposed method, compared with other techniques. © 1992-2012 IEEE.},
   author = {Bolan Su and Shijian Lu and Chew Lim Tan},
   doi = {10.1109/TIP.2012.2231089},
   issn = {10577149},
   issue = {4},
   journal = {IEEE Transactions on Image Processing},
   keywords = {Adaptive image contrast,degraded document image binarization,document analysis,document image processing,pixel classification},
   pages = {1408-1417},
   title = {Robust document image binarization technique for degraded document images},
   volume = {22},
   url = {https://www.researchgate.net/publication/233879928_A_Robust_Document_Image_Binarization_Technique_for_Degraded_Document_Images},
   year = {2013},
}
@article{Breuel2013,
   abstract = {Long Short-Term Memory (LSTM) networks have yielded excellent results on handwriting recognition. This paper describes an application of bidirectional LSTM networks to the problem of machine-printed Latin and Fraktur recognition. Latin and Fraktur recognition differs significantly from handwriting recognition in both the statistical properties of the data, as well as in the required, much higher levels of accuracy. Applications of LSTM networks to handwriting recognition use two-dimensional recurrent networks, since the exact position and baseline of handwritten characters is variable. In contrast, for printed OCR, we used a one-dimensional recurrent network combined with a novel algorithm for baseline and x-height normalization. A number of databases were used for training and testing, including the UW3 database, artificially generated and degraded Fraktur text and scanned pages from a book digitization project. The LSTM architecture achieved 0.6% character-level test-set error on English text. When the artificially degraded Fraktur data set is divided into training and test sets, the system achieves an error rate of 1.64%. On specific books printed in Fraktur (not part of the training set), the system achieves error rates of 0.15% (Fontane) and 1.47% (Ersch-Gruber). These recognition accuracies were found without using any language modelling or any other post-processing techniques. © 2013 IEEE.},
   author = {Thomas M. Breuel and Adnan Ul-Hasan and Mayce Ali Al-Azawi and Faisal Shafait},
   doi = {10.1109/ICDAR.2013.140},
   issn = {15205363},
   journal = {Proceedings of the International Conference on Document Analysis and Recognition, ICDAR},
   keywords = {LSTM Networks,OCR,RNN},
   pages = {683-687},
   title = {High-performance OCR for printed english and fraktur using lstm networks},
   year = {2013},
}
@article{,
   title = {Tesseract Blends Old and New OCR Technology-DAS2016 Tutorial-Santorini-Greece Tesseract Blends Old and New OCR Technology-DAS2016 Tutorial-Santorini-Greece 3. Features and Character Classifier Background: Classical character classification},
}
@article{Brisinello2017,
   abstract = {Efficient Optical Character Recognition (OCR) in images grabbed from Set-Top Boxes (STBs) plays an important role in STB testing. However, running OCR software on such images usually ends with low OCR performance since images can have low resolution, low image quality or colorful background. In order to improve OCR performance, four different image preprocessing methods are proposed. In this paper OCR is performed with Tesseract 3.5 and the relatively new Tesseract 4.0 on the images grabbed from different STBs. On the original images Tesseract 3.5 provides a 35.7% accuracy while Tesseract 4.0 attains a 70.2% accuracy. The proposed preprocessing methods improve OCR performance by 33.3% for Tesseract 3.5 and 22.6% for Tesseract 4.0 on the available images.},
   author = {Matteo Brisinello and Ratko Grbic and Matija Pul and Tihomir Andelic},
   doi = {10.23919/ELMAR.2017.8124460},
   isbn = {9789531842303},
   issn = {13342630},
   journal = {Proceedings Elmar - International Symposium Electronics in Marine},
   keywords = {Image Preprocessing,Low Quality Images,OCR,Tesseract},
   month = {11},
   pages = {167-171},
   publisher = {Croatian Society Electronics in Marine - ELMAR},
   title = {Improving optical character recognition performance for low quality images},
   volume = {2017-September},
   year = {2017},
}
@article{Hassanein2015,
   abstract = {For more than half a century, the Hough transform is ever-expanding for new frontiers. Thousands of research papers and numerous applications have evolved over the decades. Carrying out an all-inclusive survey is hardly possible and enormously space-demanding. What we care about here is emphasizing some of the most crucial milestones of the transform. We describe its variations elaborating on the basic ones such as the line and circle Hough transforms. The high demand for storage and computation time is clarified with different solution approaches. Since most uses of the transform take place on binary images, we have been concerned with the work done directly on gray or color images. The myriad applications of the standard transform and its variations have been classified highlighting the up-to-date and the unconventional ones. Due to its merits such as noise-immunity and expandability, the transform has an excellent history, and a bright future as well.},
   author = {Allam Shehata Hassanein and Sherien Mohammad and Mohamed Sameer and Mohammad Ehab Ragab},
   keywords = {Color,Gray-Scale,Hough Transform,Memory Saving,Shapes,Speedup},
   title = {A Survey on Hough Transform, Theory, Techniques and Applications},
}
@misc{Leptonica,
   title = {Leptonica: Leptonica Reference Documentation},
   url = {https://tpgit.github.io/Leptonica/index.html},
}
@article{Sauvola1997,
   abstract = {A new method is presented for adaptive document image binarization, where the page is considered as a collection of subcomponents such as text, background and picture. The problems caused by noise, illumination and many source type related degradations are addressed. The algorithm uses document characteristics to determine (surface) attributes, often used in document segmentation. Using characteristics analysis, two new algorithms are applied to determine a local threshold for each pixel. An algorithm based on soft decision control is used for thresholding background and picture regions. An approach utilizing local mean and variance of gray values is applied to textual regions. Tests were performed with images including different types of document components and degradations. The results show that the method adapts and performs well in each case.},
   author = {Jaakko Sauvola and Tapio Seppanen and Sami Haapakoski and Matti Pietikainen},
   doi = {10.1109/ICDAR.1997.619831},
   journal = {Proceedings of the International Conference on Document Analysis and Recognition, ICDAR},
   pages = {147-152},
   publisher = {IEEE},
   title = {Adaptive document binarization},
   volume = {1},
   year = {1997},
}
@misc{Ghostscript,
   title = {Ghostscript},
   url = {https://www.ghostscript.com/},
}
@misc{Leptonica,
   title = {Leptonica: Leptonica Reference Documentation},
   url = {https://tpgit.github.io/Leptonica/index.html},
}
@misc{Tesseract,
   title = {tesseract-ocr/tesseract: Tesseract Open Source OCR Engine (main repository)},
   url = {https://github.com/tesseract-ocr/tesseract/},
}
@article{Trier1996,
   abstract = {This paper presents an overview of feature extraction methods for off-line recognition of segmented (isolated) characters. Selection of a feature extraction method is probably the single most important factor in achieving high recognition performance in character recognition systems. Different feature extraction methods are designed for different representations of the characters, such as solid binary characters, character contours, skeletons (thinned characters) or gray-level subimages of each individual character. The feature extraction methods are discussed in terms of invariance properties, reconstructability and expected distortions and variability of the characters. The problem of choosing the appropriate feature extraction method for a given application is also discussed. When a few promising feature extraction methods have been identified, they need to be evaluated experimentally to find the best method for the given application.},
   author = {Øivind Due Trier and Anil K. Jain and Torfinn Taxt},
   doi = {10.1016/0031-3203(95)00118-2},
   issn = {0031-3203},
   issue = {4},
   journal = {Pattern Recognition},
   keywords = {Character representation,Feature extraction,Invariance,Optical character recognition,Reconstructability},
   month = {4},
   pages = {641-662},
   publisher = {Pergamon},
   title = {Feature extraction methods for character recognition-A survey},
   volume = {29},
   year = {1996},
}
@misc{Ocular,
   title = {tberg12/ocular: Ocular is a state-of-the-art historical OCR system.},
   url = {https://github.com/tberg12/ocular/},
}
@article{Levenshtein1965,
   author = {Vladimir I. Levenshtein},
   journal = {Doklady Akademii Nauk SSSR},
   pages = {845-848},
   title = {Binary codes capable of correcting deletions, insertions, and reversals},
   volume = {163},
   year = {1965},
}
@misc{binarization-image,
   title = {Let’s Take Things Step by Step, Shall We? | How OCR Works},
   url = {https://how-ocr-works.com/OCR/OCR.html},
}
@article{Noaman2015,
   author = {Khaled M. G. Noaman and Jamil Abdulhameed M. Saif and Ibrahim A. A. Alqubati},
   title = {Optical Character Recognition Based on Genetic Algorithms},
   year = {2015},
}
@inproceedings{Wang2012,
   author = {Tao Wang and David J. Wu and Adam Coates and Andrew Y. Ng},
   booktitle = {Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012)},
   pages = {3304-3308},
   title = {End-to-end text recognition with convolutional neural networks},
   url = {https://ieeexplore.ieee.org/document/6460871},
   year = {2012},
}
@article{Dharmale2023,
   abstract = {An automated reminder mechanism is built in this Android-based application. It emphasizes the contact between doctors and patients. Patients can set a reminder to remind them when it is time to take their medicine. Multiple medications and timings, including date, time, and medicine description, can be programmed into the reminder by using image processing. Patients will be notified through a message within the system, as preferred by the patients. They have the option of looking for a doctor for assistance. In this COVID-19 pandemic situation where nurses have to remind the patients in the hospitals to take their medications, our application can be useful, alerting the patient every time of the day when he/she has to take the medicine and in what amounts. Also, all the necessary tests report and prescriptions can be saved on the cloud for later use. Patients will be provided with doctor contact information based on their availability. Also, patients will be notified of the expiry date of the medicine, and the former history of the medicines can be stored for further reference. The proposed system prioritizes a good user interface and easy navigation. Image processing will be accurate and efficient with the help of powerful CNN-RNN-CTC algorithm. It also emphasizes on a secure storage of the user’s data with the help of the RSA algorithm for encryption and the gravitational search algorithm for secure cloud access. We attempted to create a Medical Reminder System that is cost-effective, time-saving, and promotes medication adherence.},
   author = {Gulbakshi Dharmale and Pratiksha Shirsath and Abhishek Shinde and Vishwajeet Sawant and Aditi Chougule},
   doi = {10.1007/978-981-19-6088-8_25},
   isbn = {9789811960871},
   issn = {23673389},
   journal = {Lecture Notes in Networks and Systems},
   keywords = {Android-based application,CNN-RNN-CTC algorithm,COVID-19,Cloud,Image processing,Medication adherence,RSA algorithm,Reminder system},
   pages = {273-283},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {REMICARE—Medicine Intake Tracker and Healthcare Assistant},
   volume = {540},
   year = {2023},
}
@misc{,
   abstract = {This paper proposes a combination of a convolutional and a LSTM network to improve the accuracy of OCR on early printed books. While the standard model of line based OCR uses a single LSTM layer, we utilize a CNN-and Pooling-Layer combination in advance of an LSTM layer. Due to the higher amount of trainable parameters the performance of the network relies on a high amount of training examples to unleash its power. Hereby, the error is reduced by a factor of up to 44%, yielding a CER of 1% and below. To further improve the results we use a voting mechanism to achieve character error rates (CER) below 0.5%. The runtime of the deep model for training and prediction of a book behaves very similar to a shallow network.},
   title = {Improving OCR Accuracy on Early Printed Books using Deep Convolutional Networks},
   url = {https://github.com/tesseract-ocr},
}
@misc{Rijhwani2022,
   author = {Shruti Rijhwani and Graham Neubig and Alan Black and Taylor Berg-Kirkpatrick and Daisy Rosenblum},
   title = {Improving Optical Character Recognition for Endangered Languages},
   year = {2022},
}
@article{Elagouni2014,
   abstract = {Text embedded in multimedia documents represents an important semantic information that helps to automatically access the content. This paper proposes two neural-based optical character recognition (OCR) systems that handle the text recognition problem in different ways. The first approach segments a text image into individual characters before recognizing them, while the second one avoids the segmentation step by integrating a multi-scale scanning scheme that allows to jointly localize and recognize characters at each position and scale. Some linguistic knowledge is also incorporated into the proposed schemes to remove errors due to recognition confusions. Both OCR systems are applied to caption texts embedded in videos and in natural scene images and provide outstanding results showing that the proposed approaches outperform the state-of-the-art methods. © 2013 Springer-Verlag Berlin Heidelberg.},
   author = {Khaoula Elagouni and Christophe Garcia and Franck Mamalet and Pascale Sébillot},
   doi = {10.1007/S10032-013-0202-7},
   issn = {14332833},
   issue = {1},
   journal = {International Journal on Document Analysis and Recognition},
   keywords = {Character segmentation,Convolutional neural network,Language model,OCR},
   month = {3},
   pages = {19-31},
   title = {Text recognition in multimedia documents: A study of two neural-based OCRs using and avoiding character segmentation},
   volume = {17},
   year = {2014},
}
@article{Kneser1995,
   abstract = {In stochastic language modeling, backing-off is a widely used method to cope with the sparse data problem. In case of unseen events this method backs off to a less specific distribution. In this paper we propose to use distributions which are especially optimized for the task of backing-off. Two different theoretical derivations lead to distributions which are quite different from the probability distributions that are usually used for backing-off. Experiments show an improvement of about 10% in terms of perplexity and 5% in terms of word error rate.},
   author = {Reinhard Kneser and Hermann Ney},
   doi = {10.1109/ICASSP.1995.479394},
   issn = {07367791},
   journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
   pages = {181-184},
   publisher = {IEEE},
   title = {Improved backing-off for M-gram language modeling},
   volume = {1},
   year = {1995},
}
@misc{IBM,
   title = {Deep learning architectures - IBM Developer},
   url = {https://developer.ibm.com/articles/cc-machine-learning-deep-learning-architectures/},
}
@misc{B130625cs2016,
   author = {Akhil S B130625cs},
   keywords = {()},
   title = {An overview of Tesseract OCR Engine A Seminar Report},
   year = {2016},
}
@misc{,
   author = {Akhil S},
   title = {An overview of Tesseract OCR Engine},
   url = {https://www.academia.edu/32328825/An_overview_of_Tesseract_OCR_Engine},
}
@article{Boiangiu2016,
   author = {Costin-Anton Boiangiu and Radu Ioanitescu and Razvan-Costin Dragomir},
   journal = {Journal of Information Systems and Operations Management},
   month = {9},
   pages = {470-486},
   title = {VOTING-BASED OCR SYSTEM},
   volume = {10},
   year = {2016},
}
@article{Rijhwani2020,
   abstract = {There is little to no data available to build natural language processing models for most endangered languages. However, textual data in these languages often exists in formats that are not machine-readable, such as paper books and scanned images. In this work, we address the task of extracting text from these resources. We create a benchmark dataset of transcriptions for scanned books in three critically endangered languages and present a systematic analysis of how general-purpose OCR tools are not robust to the data-scarce setting of endangered languages. We develop an OCR post-correction method tailored to ease training in this data-scarce setting, reducing the recognition error rate by 34% on average across the three languages.},
   author = {Shruti Rijhwani and Antonios Anastasopoulos and Graham Neubig},
   doi = {10.18653/V1/2020.EMNLP-MAIN.478},
   isbn = {9781952148606},
   journal = {EMNLP 2020 - 2020 Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference},
   pages = {5931-5942},
   publisher = {Association for Computational Linguistics (ACL)},
   title = {OCR Post Correction for Endangered Language Texts},
   url = {https://aclanthology.org/2020.emnlp-main.478},
   year = {2020},
}
@article{Springmann2017,
   abstract = {This article describes the results of a case study that applies Neural Network-based Optical Character Recognition (OCR) to scanned images of books printed between 1487 and 1870 by training the OCR engine OCRopus [Breuel et al. 2013] on the RIDGES herbal text corpus [Odebrecht et al. 2017] (in press). Training specific OCR models was possible because the necessary ground truth is available as error-corrected diplomatic transcriptions. The OCR results have been evaluated for accuracy against the ground truth of unseen test sets. Character and word accuracies (percentage of correctly recognized items) for the resulting machine-readable texts of individual documents range from 94% to more than 99% (character level) and from 76% to 97% (word level). This includes the earliest printed books, which were thought to be inaccessible by OCR methods until recently. Furthermore, OCR models trained on one part of the corpus consisting of books with different printing dates and different typesets (mixed models) have been tested for their predictive power on the books from the other part containing yet other fonts, mostly yielding character accuracies well above 90%. It therefore seems possible to construct generalized models trained on a range of fonts that can be applied to a wide variety of historical printings still giving good results. A moderate postcorrection effort of some pages will then enable the training of individual models with even better accuracies. Using this method, diachronic corpora including early printings can be constructed much faster and cheaper than by manual transcription. The OCR methods reported here open up the possibility of transforming our printed textual cultural heritage into electronic text by largely automatic means, which is a prerequisite for the mass conversion of scanned books.},
   author = {Uwe Springmann and Anke Lüdeling},
   issue = {2},
   journal = {Digital Humanities Quarterly},
   title = {OCR of historical printings with an application to building diachronic corpora: A case study using the RIDGES herbal corpus},
   volume = {11},
   year = {2017},
}
